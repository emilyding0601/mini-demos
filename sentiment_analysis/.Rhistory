dinner
# Use double-bracket notation to extract your `lunch` element from your list
# and save it in your list as the element at index 5 (no reason beyond practice)
meals[[5]] <- meals[['lunch']]
meals[[5]]
# Use single-bracket notation to extract your breakfast and lunch from your list
# and save them to a list called `early_meals`
early_meals <- meals[1:2]
early_meals
early_meals <- meals[1:2]
early_meals
# vector of names
name <- c("Ada", "Bob", "Chris", "Diya", "Emma")
# Vector of heights
height <- 58:62
# Vector of weights
weight <- c(115, 117, 120, 123, 126)
# Combine the vectors into a data.frame
# Note the names of the variables become the names of the columns!
my_data <- data.frame(name, height, weight, stringsAsFactors = FALSE)
my_data
view(my_data)
View(my_data)
dim(my_data)
employees <- paste('Employee', 1:100)
employees
random_salaries <- runif(40000, 50000, 100)
random_salaries
random_salaries <- runif(100, 40000, 50000)
random_salaries
next_year_salaries <- runif(100, -5000, 10000, random_salaries)
next_year_salaries
# salaries (the negative number means that a salary may have decreased!)
next_year_salaries <- random_salaries + runif(100, -5000, 10000)
next_year_salaries
salaries <- data.frame(employees, random_salaries, next_year_salaries, stringsAsFactors = FALSE)
salaries
salaries$change <- random_salaries - next_year_salaries
salaries
salaries$got_raise <- salaries$change > 0
salaries
salaries[salaries$employees == "Employee 57", "next_year_salaries"]
employee_57 <- salaries[salaries$employees == "Employee 57", "next_year_salaries"]
employee_57
install.packages('iris')
library(iris)
View(iris)
iris$Sepal.Length
# Select only rows of Virginica flowers
iris[iris$Species == 'virginica', ]
virginica <- iris[iris$Species == 'virginica', ]
# Select only the sepal legnth of the Virginica flowers
virginica$Sepal.Length
virginica <- iris[iris$Species == 'virginica', ]
virginica
virginica$Sepal.Length
# Select rows where Petal.Length > 4.0
iris[Petal.Length > 4.0, ]
# Select rows where Petal.Length > 4.0
iris[iris$Petal.Length > 4.0, ]
greater_4 <- iris[iris$Petal.Length > 4.0, ]
greater_4
iris$greater_4 <- iris[iris$Petal.Length > 4.0, ]
iris&greater_4
iris$greater_4 <- iris[iris$Petal.Length > 4.0, ]
iris$greater_4
iris$greater_4 <- iris$Petal.Length > 4.0
iris$greater_4
# Select all values of data frame for Sepal.length
iris$Sepal.Length
iris[, 'Sepal.Length']
# Select only rows of Virginica flowers
virginica <- iris[iris$Species == 'virginica', ]
# Select only the sepal length of the Virginica flowers
virginica$Sepal.Length
# Select rows of iris where Petal.Length > 4.0
iris[iris$Petal.Length > 4, ]
# Add a column to the dataframe indicating whether Petal.Length > 4.0
iris$petal_length_greater <- iris$Petal.Length > 4.0
# Find the species type of the flower that has the longest Sepal length
iris[iris$Sepal.Length == max(iris$Sepal.Length), ]$Species
# View rows where species type is 'setosa' and Sepal.Width > 3.0
View(iris[((iris$Species == 'setosa') & (iris$Sepal.Width > 3.0)), ])
employee_57 <- salaries[salaries$employees == "Employee 57", "next_year_salaries"]
employee_57
salaries[salaries$got_raise]
employee_57 <- salaries[salaries$employees == "Employee 57", "next_year_salaries"]
employee_57
# How many employees got a raise?
count_raise <- salaries[salaries$got_raise]
count_raise
employees <- paste('Employee', 1:100)
employees
# Create a vector of 100 random salaries for the year 2017
# Use the `runif()` function to pick random numbers between 40000 and 50000
random_salaries <- runif(100, 40000, 50000)
random_salaries
# Create a vector of 100 salaries in 2018 that have changed from 2017
# Use `runif()` to add a random number between -5000 and 10000 to each of 2017's
# salaries (the negative number means that a salary may have decreased!)
next_year_salaries <- random_salaries + runif(100, -5000, 10000)
next_year_salaries
# Create a data frame 'salaries' by combining the 3 vectors you just made
# Remember to set `stringsAsFactors=FALSE`!
salaries <- data.frame(employees, random_salaries, next_year_salaries, stringsAsFactors = FALSE)
salaries
# Create a column 'change' that stores each person's change in salary between
# 2017 and 2018
salaries$change <- random_salaries - next_year_salaries
salaries
# Create a column 'got_raise' that is TRUE if the person got a raise (their
# salary went up)
salaries$got_raise <- salaries$change > 0
salaries
### Retrieve values from your data frame to answer the following questions
### Note that you should get the value as specific as possible (e.g., a single
### cell rather than the whole row!)
# What was the 2018 salary of employee 57
employee_57 <- salaries[salaries$employees == "Employee 57", "next_year_salaries"]
employee_57
# How many employees got a raise?
count_raise <- salaries[salaries$got_raise]
count_raise
nrow(salaries) <- salaries[salaries$got_raise]
nrow(salaries) <- salaries(salaries$got_raise)
install.packages("dplyr")  # once per machine
library("dplyr")
# Use the function to compare the life expectancy changes of the "United States" and "Cuba"
# Store the data frame in a variable "us_vs_cuba"
us_vs_cuba <- compare_countries("United States","Cuba")
us_vs_cuba
# Extract the data from outside csv file, named as "life_expectancy"
life_expectancy <- read.csv("data/life_expectancy.csv", stringsAsFactors = FALSE)
View(life_expectancy)
# Add a column called change to the data frame that is the change in life expectancy from 1960 to 2013
life_expectancy$change <- life_expectancy$le_2013 - life_expectancy$le_1960
# Create a variable "num_small_gain" that has the number of countries
# whose life expectancy has not improved by 5 years or more between 1960 and 2013.
# Print out this variable.
num_small_gain <- length(life_expectancy$country[life_expectancy$change <= 5])
print(num_small_gain)
# Create a variable "most_improved" that is the name of the country with the largest gain in life expectancy.
# Print out this variable.
most_improved <- life_expectancy$country[life_expectancy$change == max(life_expectancy$change)]
print(most_improved)
# Define a function called "country_change" that takes in a country's name as an argument,
# and returns that country's change in life expectancy from 1960 to 2013.
country_change <- function(country_name) {
life_expectancy$change[life_expectancy$country == country_name]
}
# Print out the change in life expectancy from 1960 to 2013 in "Haiti."
print(country_change("Haiti"))
# Define a function called "lowest_life_exp_in_region" that takes in a region name as an argument,
# and returns the name of the country with the lowest life expectancy in 2013 (in that region).
lowest_life_exp_in_region <- function(region_name) {
spec_region <- life_expectancy[life_expectancy$region == region_name, ]
spec_region$country[spec_region$le_2013 == min(spec_region$le_2013)]
}
# Print out the country with the lowest life expectancy in 2013 in the "Latin America & Caribbean" region
print(lowest_life_exp_in_region("Latin America & Caribbean"))
# Define a function called "compare_countries" that takes in two country names as arguments
# Returns a table of those two country's life expectancies in 2013 and the change since 1960.
compare_countries <- function(country_n1, country_n2) {
country <- c(country_n1, country_n2)
le_2013 <- c(life_expectancy$le_2013[life_expectancy$country == country_n1],
life_expectancy$le_2013[life_expectancy$country == country_n2])
changes <- c(life_expectancy$change[life_expectancy$country == country_n1],
life_expectancy$change[life_expectancy$country == country_n2])
data.frame(country, le_2013, changes, stringsAsFactors = FALSE)
}
# Use the function to compare the life expectancy changes of the "United States" and "Cuba"
# Store the data frame in a variable "us_vs_cuba"
us_vs_cuba <- compare_countries("United States","Cuba")
us_vs_cuba
Extract the data from outside csv file, named as "life_expectancy"
life_expectancy <- read.csv("data/life_expectancy.csv", stringsAsFactors = FALSE)
View(life_expectancy)
# Add a column called change to the data frame that is the change in life expectancy from 1960 to 2013
life_expectancy$change <- life_expectancy$le_2013 - life_expectancy$le_1960
# Create a variable "num_small_gain" that has the number of countries
# whose life expectancy has not improved by 5 years or more between 1960 and 2013.
# Print out this variable.
num_small_gain <- length(life_expectancy$country[life_expectancy$change <= 5])
print(num_small_gain)
# Create a variable "most_improved" that is the name of the country with the largest gain in life expectancy.
# Print out this variable.
most_improved <- life_expectancy$country[life_expectancy$change == max(life_expectancy$change)]
print(most_improved)
# Define a function called "country_change" that takes in a country's name as an argument,
# and returns that country's change in life expectancy from 1960 to 2013.
country_change <- function(country_name) {
life_expectancy$change[life_expectancy$country == country_name]
}
# Print out the change in life expectancy from 1960 to 2013 in "Haiti."
print(country_change("Haiti"))
# Define a function called "lowest_life_exp_in_region" that takes in a region name as an argument,
# and returns the name of the country with the lowest life expectancy in 2013 (in that region).
lowest_life_exp_in_region <- function(region_name) {
spec_region <- life_expectancy[life_expectancy$region == region_name, ]
spec_region$country[spec_region$le_2013 == min(spec_region$le_2013)]
}
# Print out the country with the lowest life expectancy in 2013 in the "Latin America & Caribbean" region
print(lowest_life_exp_in_region("Latin America & Caribbean"))
# Define a function called "compare_countries" that takes in two country names as arguments
# Returns a table of those two country's life expectancies in 2013 and the change since 1960.
compare_countries <- function(country_n1, country_n2) {
country <- c(country_n1, country_n2)
le_2013 <- c(life_expectancy$le_2013[life_expectancy$country == country_n1],
life_expectancy$le_2013[life_expectancy$country == country_n2])
changes <- c(life_expectancy$change[life_expectancy$country == country_n1],
life_expectancy$change[life_expectancy$country == country_n2])
data.frame(country, le_2013, changes, stringsAsFactors = FALSE)
}
# Use the function to compare the life expectancy changes of the "United States" and "Cuba"
# Store the data frame in a variable "us_vs_cuba"
us_vs_cuba <- compare_countries("United States","Cuba")
us_vs_cuba
install.packages("devtools")
# Install "fueleconomy" dataset from GitHub
devtools::install_github("hadley/fueleconomy")
# Use the `libary()` function to load the "fueleconomy" package
library(fueleconomy)
# You can use `View()` to inspect it
View(vehicles)
library("dplyr")
library(dplyr)
View(vehicles)
vehicle_maker <- vehicles$make
sum_maker <- sum(unique(vehicle_maker))
sum_maker <- length(unique(vehicle_maker))
sum_maker
# Filter the data set for vehicles manufactured in 1997
filter_1997 <- vehicles[vehicles$year == "1997", ]
filter_1997
cars_1997 <- filter_1997[order(filter_1997$hwy),]
cars_1997
cars_1997 <- filter_1997[order(filter_1997$hwy),]
cars_1997
# Mutate the 1997 cars data frame to add a column `average` that has the average
# gas milage (between city and highway mpg) for each car
cars_1997$average = (cars_1997$hwy + cars_1997$cty) / 2
cars_1997$average
response
library(httr)
library(jsonlite)
source("api_key.R")
base_uri <- "https://api.propublica.org/congress/v1/"
base_uri <- "https://api.propublica.org/congress/v1/"
response <- GET(paste0(base_uri, "bills/subjects.json"), add_headers("X-API-KEY" = propublica_key))
response
base_uri <- "https://api.propublica.org/congress/v1/"
response <- GET(paste0(base_uri, "bills/subjects/immigration.json"), add_headers("X-API-KEY" = propublica_key))
immigration_body <- content(response, "text")
parsed_imm_body <- fromJSON(immigration_body)
recent_bill <- data.frame(parsed_imm_body$results, stringsAsFactors = FALSE)
recent_bill <- flatten(recent_bill)
recent_bill
recent_bill
recent_bill
library(httr)
library(jsonlite)
library(dyplr)
library(httr)
library(jsonlite)
library(dplyr)
source("api_key.R")
topic <- "immigration"
base_uri <- "https://api.propublica.org/congress/v1/"
response <- GET(paste0(base_uri, "bills/subjects/", topic, ".json"), add_headers("X-API-KEY" = propublica_key))
immigration_body <- content(response, "text")
parsed_imm_body <- fromJSON(immigration_body)
recent_bill <- data.frame(parsed_imm_body$results, stringsAsFactors = FALSE)
recent_bill <- flatten(recent_bill)
recent_bill
recent_bill
most_recent_bill <- arrange(recent_bill, -latest_major_action_date)
recent_bill
is.data.frame(recent_bill)
knitr::opts_chunk$set(echo = TRUE)
library(httr)
library(jsonlite)
library(dplyr)
source("api_key.R")
topic <- "immigration"
base_uri <- "https://api.propublica.org/congress/v1/"
response <- GET(paste0(base_uri, "bills/subjects/", topic, ".json"), add_headers("X-API-KEY" = propublica_key))
immigration_body <- content(response, "text")
parsed_imm_body <- fromJSON(immigration_body)
recent_bill <- data.frame(parsed_imm_body$results, stringsAsFactors = FALSE)
recent_bill <- flatten(recent_bill)
most_recent_bill <- arrange(recent_bill, -latest_major_action_date)
most_recent_bill <- arrange(recent_bill, desc(latest_major_action_date))
most_recent_bill
most_recent_bill <- arrange(recent_bill, desc(latest_major_action_date)) %>%  head(10)
most_recent_bill
top_ten_bill <- head(most_recent_bill, 10)
top_ten_bill
most_recent_bill <- arrange(recent_bill, desc(latest_major_action_date)) %>% head(10)
most_recent_bill
most_recent_bill <- arrange(recent_bill, desc(latest_major_action_date)) %>% head(10) %>% select(bill_uri, title, short_title, sponsor_name, sponsor_state, sponsor_party, latest_major_action_date, latest_major_action, active, govtrack_url)
most_recent_bill
library("ggplot2")
df <- data.frame(
variable = c("does not resemble", "resembles"),
value = c(20, 80)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("red", "yellow")) +
coord_polar("y", start = 2*pi/3) +
labs(title = "Pac Man")
knitr::opts_chunk$set(echo = TRUE)
library("ggplot2")
ggplot((df, aes(x = "", y = value, fill = variable)) +
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("red", "yellow"))
# Load the Shiny library
library(shiny)
library(ggplot2)
runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/exercises-emilyding0601/ch16-shiny/exercise-4')
runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/exercises-emilyding0601/ch16-shiny/exercise-4')
runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/exercises-emilyding0601/ch16-shiny/exercise-4')
mpg <- data(mpg)
mpg
?mpg
runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/exercises-emilyding0601/ch16-shiny/exercise-4')
library(shiny)
library(ggplot2)
runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/exercises-emilyding0601/ch16-shiny/exercise-4')
runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/exercises-emilyding0601/ch16-shiny/exercise-4')
library(shiny)
library(ggplot2)
runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/exercises-emilyding0601/ch16-shiny/exercise-4')
runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/exercises-emilyding0601/ch16-shiny/exercise-4')
runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/exercises-emilyding0601/ch16-shiny/exercise-4')
runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/exercises-emilyding0601/ch16-shiny/exercise-4')
runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/exercises-emilyding0601/ch16-shiny/exercise-4')
runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/exercises-emilyding0601/ch16-shiny/exercise-4')
is.numeric(mpg)
mpg <- as.numeric(mpg)
runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/exercises-emilyding0601/ch16-shiny/exercise-4')
shiny::runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/Assg/a8-data-app-emilyding0601')
shiny::runApp('C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/Assg/a8-data-app-emilyding0601')
21/24
14/24
21/24 -14/24
promo <- function(i) {
m <- rbinom(24, 1, 35/48)
w <- rbinom(24, 1, 35/48)
mean(m) - mean(w)
}
dif <- sapply(1:10000, promo)
head(dif)
MASS::truehist(his)
mean(dif <- -.029 | dif > 0.29)
promo <- function(i) {
m <- rbinom(24, 1, 35/48)
w <- rbinom(24, 1, 35/48)
mean(m) - mean(w)
}
dif <- sapply(1:10000, promo)
head(dif)
MASS::truehist(dif)
mean(dif <- -.029 | dif > 0.29)
install.packages('rvest')
library('rvest')
url <- 'http://www.imdb.com/search/title?count=100&release_date=2016,2016&title_type=feature'
webpage <- read_html(url)
'Specify the URL endpoint we are using'
url <- 'http://www.imdb.com/search/title?count=100&release_date=2016,2016&title_type=feature'
webpage <- read_html(url)
#html_nodes: More easily extract pieces out of HTML documents using XPath and css selectors
#html_text: Extract attributes, text and tag name from html.
rank_data_html <- html_nodes(webpage,'.text-primary')
rank_data <- html_text(rank_data_html)
head(rank_data)
library('rvest')
'Specify the URL endpoint we are using'
url <- 'http://www.imdb.com/search/title?count=100&release_date=2016,2016&title_type=feature'
webpage <- read_html(url)
url <- 'http://www.imdb.com/search/title?count=100&release_date=2016,2016&title_type=feature'
webpage <- read_html(url)
rank_data_html <- html_nodes(webpage,'.text-primary')
rank_data <- html_text(rank_data_html)
head(rank_data)
head(rank_data)
rank_data<-as.numeric(rank_data)
head(rank_data)
title_data_html <- html_nodes(webpage, '.lister-item-header a')
#html to text
title_data <- html_text(title_data_html)
#look at data
head(title_data)
des_data_html <- html_nodes(webpage, '.ratings-bar+ .text-muted')
des_data <- html_text(des_data_html)
head(des_data)
#Data-Preprocessing: removing '\n'
des_data <- gsub("\n","",des_data)
#html to text
title_data <- html_text(title_data_html)
#look at data
head(title_data)
#Using CSS selectors to scrape the description section
des_data_html <- html_nodes(webpage, '.ratings-bar+ .text-muted')
#Converting the description data to text
des_data <- html_text(des_data_html)
#look at data
head(des_data)
#Data-Preprocessing: removing '\n'
des_data <- gsub("\n","",des_data)
#Using CSS selectors to scrap the Movie runtime section
runtime_data_html <- html_nodes(webpage,'.text-muted .runtime')
#Converting the movie runtime data to text
runtime_data <- html_text(runtime_data_html)
#Let's have a look at the movie runtime
head(runtime_data)
#Data-Preprocessing: removing mins and converting it to numerical
runtime_data <- gsub(" min","",runtime_data)
runtime_data <- as.numeric(runtime_data)
#Let's have another look at the runtime data
head(runtime_data)
install.packages('syuzhet')
library(syuzhet)
# Add some happy ones, angry ones - you name it!
student_sentences <- c('I really like the pie you gave me this morning.',
'Your shoes suck and are just plain ugly.',
'I\'d really truly love going out in this weather!'
)
# Analyze sentiment for student sentences
student_sentiments <- data.frame(get_sentiment(student_sentences, method = 'syuzhet'))
student_analysis <- cbind(sentence = student_sentences, student_sentiments)
View(student_analysis)
library(dplyr)
library(stringr)
library(tidytext)
library(tidyr)
library(ggplot2)
View(get_sentiment('nrc'))
setwd("C:/Users/lovel/Google Drive/UW/INFO/Spring 2018/INFO 201/Assg/mini-demos/sentiment_analysis")
bing_sentiments <- get_sentiment('bing')
##### DATA ANALYSIS + WRANGLING #####
# Read books data in
books <- read.csv('./data/austen_books.csv', stringsAsFactors = FALSE)
View(books)
jane_austen_sentiments <- books %>%
inner_join(bing_sentiments, by = 'word')
View(jane_austen_sentiments)
# Map each word in the 'books' dataset to its dictionary-prescribed sentiment.
jane_austen_sentiments <- books %>%
inner_join(bing_sentiments, by = 'word')
View(jane_austen_sentiments)
jane_austen_sentiments <- books %>%
inner_join(bing_sentiments, by = 'word')
bing_sentiments <- get_sentiment('bing')
books <- read.csv('./data/austen_books.csv', stringsAsFactors = FALSE)
View(books)
install.packages('tidytext')
library(dplyr)
library(stringr)
library(tidytext)
library(tidyr)
library(ggplot2)
jane_austen_sentiments <- books %>%
inner_join(bing_sentiments, by = 'word')
# positive and negative.
bing_sentiments <- get_sentiment('bing')
##### DATA ANALYSIS + WRANGLING #####
# Read books data in
books <- read.csv('./data/austen_books.csv', stringsAsFactors = FALSE)
View(books)
# Map each word in the 'books' dataset to its dictionary-prescribed sentiment.
jane_austen_sentiments <- books %>%
inner_join(bing_sentiments, by = 'word')
View(jane_austen_sentiments)
)
install.packages('tidyr')
install.packages("tidyr")
jane_austen_sentiments <- books %>%
inner_join(bing_sentiments, by = 'word')
library(dplyr)
library(stringr)
library(tidytext)
library(tidyr)
library(ggplot2)
install.packages('tidytext')
library(tidytext)
install.packages('psych')
library(dplyr)
library(stringr)
library(tidytext)
library(tidyr)
library(ggplot2)
bing_sentiments <- get_sentiment('bing')
##### DATA ANALYSIS + WRANGLING #####
# Read books data in
books <- read.csv('./data/austen_books.csv', stringsAsFactors = FALSE)
View(books)
# Map each word in the 'books' dataset to its dictionary-prescribed sentiment.
jane_austen_sentiments <- books %>%
inner_join(bing_sentiments, by = 'word')
View(jane_austen_sentiments)
books <- read.csv('./data/austen_books.csv', stringsAsFactors = FALSE)
# Map each word in the 'books' dataset to its dictionary-prescribed sentiment.
jane_austen_sentiment <- books %>%
inner_join(bing_sentiments, by = "word")
# First, install the keras R package from GitHub as follows:
devtools::install_github("rstudio/keras")
# The Keras R interface uses the TensorFlow backend engine by default.
# To install both the core Keras library as well as the TensorFlow backend use the install_keras() function:
library(keras)
install_keras()
